{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import keras\n",
    "# import cv2\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"train/\"\n",
    "test_path = \"test/\"\n",
    "label_path = './train.csv'\n",
    "train_files = listdir(train_path)\n",
    "test_files = listdir(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = pd.read_csv(label_path)\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_dict = {\n",
    "    0:  \"Nucleoplasm\",  \n",
    "    1:  \"Nuclear membrane\",   \n",
    "    2:  \"Nucleoli\",   \n",
    "    3:  \"Nucleoli fibrillar center\",   \n",
    "    4:  \"Nuclear speckles\",\n",
    "    5:  \"Nuclear bodies\",   \n",
    "    6:  \"Endoplasmic reticulum\",   \n",
    "    7:  \"Golgi apparatus\",   \n",
    "    8:  \"Peroxisomes\",   \n",
    "    9:  \"Endosomes\",   \n",
    "    10:  \"Lysosomes\",   \n",
    "    11:  \"Intermediate filaments\",   \n",
    "    12:  \"Actin filaments\",   \n",
    "    13:  \"Focal adhesion sites\",   \n",
    "    14:  \"Microtubules\",   \n",
    "    15:  \"Microtubule ends\",   \n",
    "    16:  \"Cytokinetic bridge\",   \n",
    "    17:  \"Mitotic spindle\",   \n",
    "    18:  \"Microtubule organizing center\",   \n",
    "    19:  \"Centrosome\",   \n",
    "    20:  \"Lipid droplets\",   \n",
    "    21:  \"Plasma membrane\",   \n",
    "    22:  \"Cell junctions\",   \n",
    "    23:  \"Mitochondria\",   \n",
    "    24:  \"Aggresome\",   \n",
    "    25:  \"Cytosol\",   \n",
    "    26:  \"Cytoplasmic bodies\",   \n",
    "    27:  \"Rods & rings\"\n",
    "}\n",
    "# reverse_names_dict = dict((v,k) for k,v in names_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_targets(row):\n",
    "    row.Target = np.array(row.Target.split(\" \")).astype(np.int)\n",
    "    for num in row.Target:\n",
    "        name = names_dict[int(num)]\n",
    "        row.loc[name] = 1\n",
    "    return row\n",
    "\n",
    "for key in names_dict.keys():\n",
    "    train_labels[names_dict[key]] = 0\n",
    "train_labels = train_labels.apply(fill_targets, axis=1)\n",
    "train_labels[\"number_of_targets\"] = train_labels.drop([\"Id\", \"Target\"],axis=1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3)\n",
    "for train_idx, test_idx in kf.split(train_labels.index.values):\n",
    "    partition = {}\n",
    "    partition[\"train\"] = train_labels.Id.values[train_idx]\n",
    "    partition[\"validation\"] = train_labels.Id.values[test_idx]\n",
    "#     X_train, X_test = train_labels.Id.values[train_idx], train_labels.Id.values[test_idx]\n",
    "#     y_train, y_test = train_labels.Id.values[train_idx], train_labels.Id.values[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParameters(object):\n",
    "    path = train_path\n",
    "    num_classes=28\n",
    "    image_rows=512\n",
    "    image_cols=512\n",
    "    batch_size=100\n",
    "    n_channels=3\n",
    "    shuffle=False\n",
    "    scaled_row_dim = 139\n",
    "    scaled_col_dim = 139 \n",
    "    n_epochs=10\n",
    "\n",
    "parameter = ModelParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePreprocessor:\n",
    "    \n",
    "    def __init__(self, modelparameter):\n",
    "        self.parameter = modelparameter\n",
    "        self.path = self.parameter.path\n",
    "        self.scaled_row_dim = self.parameter.scaled_row_dim\n",
    "        self.scaled_col_dim = self.parameter.scaled_col_dim\n",
    "        self.n_channels = self.parameter.n_channels\n",
    "    \n",
    "    def preprocess(self, image):\n",
    "        image = self.resize(image)\n",
    "        image = self.reshape(image)\n",
    "        image = self.normalize(image)\n",
    "        return image\n",
    "    \n",
    "    def resize(self, image):\n",
    "        return resize(image, (self.scaled_row_dim, self.scaled_col_dim))\n",
    "    \n",
    "    def reshape(self, image):\n",
    "        return np.reshape(image, (image.shape[0], image.shape[1], self.n_channels))\n",
    "    \n",
    "    def normalize(self, image):\n",
    "#         image /= 255\n",
    "#         return image\n",
    "        return (image / 255.0 - 0.5) / 0.5\n",
    "            \n",
    "    def load_image(self, image_id):\n",
    "        image = np.zeros(shape=(512,512,4))\n",
    "        image[:,:,0] = imread(self.basepath + image_id + \"_green\" + \".png\")\n",
    "        image[:,:,1] = imread(self.basepath + image_id + \"_blue\" + \".png\")\n",
    "        image[:,:,2] = imread(self.basepath + image_id + \"_red\" + \".png\")\n",
    "        image[:,:,3] = imread(self.basepath + image_id + \"_yellow\" + \".png\")\n",
    "        return image[:,:,0:self.parameter.n_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ImagePreprocessor(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, list_IDs, labels, modelparameter, imagepreprocessor):\n",
    "        self.params = modelparameter\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.dim = (self.params.scaled_row_dim, self.params.scaled_col_dim)\n",
    "        self.batch_size = self.params.batch_size\n",
    "        self.n_channels = self.params.n_channels\n",
    "        self.num_classes = self.params.num_classes\n",
    "        self.preprocessor = imagepreprocessor\n",
    "        self.shuffle = self.params.shuffle\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def get_targets_per_image(self, identifier):\n",
    "        return self.labels.loc[self.labels.Id==identifier].drop(\n",
    "                [\"Id\", \"Target\", \"number_of_targets\"], axis=1).values\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, self.num_classes), dtype=int)\n",
    "        for i, identifier in enumerate(list_IDs_temp):\n",
    "            image = self.preprocessor.load_image(identifier)\n",
    "            image = self.preprocessor.preprocess(image)\n",
    "            X[i] = image\n",
    "            y[i] = self.get_targets_per_image(identifier)\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictGenerator:\n",
    "    \n",
    "    def __init__(self, predict_Ids, imagepreprocessor, predict_path):\n",
    "        self.preprocessor = imagepreprocessor\n",
    "        self.preprocessor.basepath = predict_path\n",
    "        self.identifiers = predict_Ids\n",
    "    \n",
    "    def predict(self, model):\n",
    "        y = np.empty(shape=(len(self.identifiers), self.preprocessor.parameter.num_classes))\n",
    "        for n in range(len(self.identifiers)):\n",
    "            image = self.preprocessor.load_image(self.identifiers[n])\n",
    "            image = self.preprocessor.preprocess(image)\n",
    "            image = image.reshape((1, *image.shape))\n",
    "            y[n] = model.predict(image)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def base_f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return f1\n",
    "\n",
    "def f1_min(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.min(f1)\n",
    "\n",
    "def f1_max(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.max(f1)\n",
    "\n",
    "def f1_mean(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_std(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.std(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackHistory(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedDataGenerator(DataGenerator):\n",
    "    \n",
    "    # in contrast to the base DataGenerator we add a target wishlist to init\n",
    "    def __init__(self, list_IDs, labels, modelparameter, imagepreprocessor, target_wishlist):\n",
    "        super().__init__(list_IDs, labels, modelparameter, imagepreprocessor)\n",
    "        self.target_wishlist = target_wishlist\n",
    "    \n",
    "    def get_targets_per_image(self, identifier):\n",
    "        return self.labels.loc[self.labels.Id==identifier][self.target_wishlist].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_submission.csv')\n",
    "partition2 = np.array(df['Id'])\n",
    "parameter = ModelParameters()\n",
    "preprocessor = ImagePreprocessor(parameter)\n",
    "labels = train_labels\n",
    "training_generator = DataGenerator(partition['train'], labels,\n",
    "                                           parameter, preprocessor)\n",
    "validation_generator = DataGenerator(partition['validation'], labels,\n",
    "                                             parameter, preprocessor)\n",
    "predict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4,\\\n",
    "        inter_op_parallelism_threads=4, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : 4, 'GPU' : 1})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import Model\n",
    "from inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.callbacks import Callback\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "def create_model(input_shape, n_out):\n",
    "    \n",
    "    pretrain_model = InceptionResNetV2(\n",
    "        include_top=False, \n",
    "        weights='imagenet', \n",
    "        input_shape=input_shape)    \n",
    "    \n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = pretrain_model(bn)\n",
    "    x = Conv2D(128, kernel_size=(1,1), activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1205: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 139, 139, 3)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 139, 139, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_resnet_v2 (Model)  (None, 3, 3, 1536)        54336736  \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 3, 3, 128)         196736    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               590336    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 28)                14364     \n",
      "=================================================================\n",
      "Total params: 55,138,184\n",
      "Trainable params: 55,077,634\n",
      "Non-trainable params: 60,550\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model = create_model(\n",
    "    input_shape=(ModelParameters.scaled_row_dim,ModelParameters.scaled_col_dim,\\\n",
    "                 ModelParameters.n_channels), \n",
    "    n_out=ModelParameters.num_classes)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 99/100 [============================>.] - ETA: 3s - loss: 0.1442 - acc: 0.9514 - f1_mean: 0.0824Epoch 00000: val_loss improved from inf to 0.15057, saving model to IRNV2_new.model\n",
      "100/100 [==============================] - 874s - loss: 0.1442 - acc: 0.9514 - f1_mean: 0.0821 - val_loss: 0.1506 - val_acc: 0.9494 - val_f1_mean: 0.0620\n",
      "Epoch 2/10\n",
      " 99/100 [============================>.] - ETA: 3s - loss: 0.1413 - acc: 0.9524 - f1_mean: 0.1009Epoch 00001: val_loss improved from 0.15057 to 0.14833, saving model to IRNV2_new.model\n",
      "100/100 [==============================] - 681s - loss: 0.1411 - acc: 0.9525 - f1_mean: 0.1017 - val_loss: 0.1483 - val_acc: 0.9507 - val_f1_mean: 0.0990\n",
      "Epoch 3/10\n",
      " 99/100 [============================>.] - ETA: 3s - loss: 0.1223 - acc: 0.9579 - f1_mean: 0.1564Epoch 00002: val_loss did not improve\n",
      "100/100 [==============================] - 768s - loss: 0.1223 - acc: 0.9579 - f1_mean: 0.1568 - val_loss: 0.1542 - val_acc: 0.9506 - val_f1_mean: 0.1166\n",
      "Epoch 4/10\n",
      " 99/100 [============================>.] - ETA: 3s - loss: 0.1234 - acc: 0.9577 - f1_mean: 0.1626"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "checkpointer = ModelCheckpoint(\n",
    "    'IRNV2_new.model',\n",
    "    verbose=2, save_best_only=True)\n",
    "\n",
    "model.load_weights('IRNV2_new.model')\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',  \n",
    "    optimizer=keras.optimizers.Adadelta(),\n",
    "    metrics=['acc', f1_mean])\n",
    "\n",
    "history = model.fit_generator(\n",
    "    training_generator,\n",
    "    steps_per_epoch=100,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=ModelParameters.n_epochs, \n",
    "    verbose=1,\n",
    "    validation_steps = 100,\n",
    "    callbacks=[checkpointer]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"InceptionResNetv2_model_preweights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_predictions = model.predict(predict_generator)\n",
    "improved_proba_predictions = pd.DataFrame(proba_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_proba_predictions.to_csv(\"InceptionResNetV2_pretrain_predictions_preweights.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
